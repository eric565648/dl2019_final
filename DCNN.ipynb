{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 1000\n",
    "\n",
    "DATA_FOLDER = 'set00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode):\n",
    "    if mode == 'train':\n",
    "        \n",
    "        img = pd.read_csv(DATA_FOLDER + '_train_img.csv')\n",
    "        label = pd.read_csv(DATA_FOLDER + '_train_label.csv')\n",
    "        return np.squeeze(img.values), np.squeeze(label.values)\n",
    "    else:\n",
    "        img = pd.read_csv(DATA_FOLDER + '_test_img.csv')\n",
    "        label = pd.read_csv(DATA_FOLDER + '_test_label.csv')\n",
    "        return np.squeeze(img.values), np.squeeze(label.values)\n",
    "\n",
    "\n",
    "class Loader(Data.Dataset):\n",
    "    def __init__(self, root, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Root path of the dataset.\n",
    "            mode : Indicate procedure status(training or testing)\n",
    "\n",
    "            self.img_name (string list): String list that store all image names.\n",
    "            self.label (int or float list): Numerical list that store all ground truth label values.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.img_name, self.label = getData(mode)\n",
    "        self.mode = mode\n",
    "        print(\"> Found %d images...\" % (len(self.img_name)))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        #------------return the size of dataset\n",
    "        return len(self.img_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #-------------Get the image path from 'self.img_name' and load it.\n",
    "                  \n",
    "        path = self.root + self.img_name[index] + '.png'\n",
    "        img = Image.open(path)\n",
    "        img_as_img = img.resize(( 160, 120),Image.ANTIALIAS)\n",
    "        \n",
    "        #-------------Get the ground truth label from self.label\"\"\"\n",
    "        label = torch.from_numpy(self.label)[index]\n",
    "        \n",
    "        #-------------Transform the .jpeg rgb images\n",
    "        if self.mode == 'train':\n",
    "            transform1 = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=(-45,45), resample=False, expand=False, center=None),\n",
    "                transforms.ColorJitter(contrast=(0,1)),\n",
    "                transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform1 = transforms.Compose([\n",
    "                transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "                ]\n",
    "            )\n",
    "        img_tran = transform1(img_as_img)\n",
    "                \n",
    "        #-------------Return processed image and label\n",
    "        return img_tran, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Found 2339 images...\n",
      "> Found 259 images...\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f01585a4c18>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f01d15946d8>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "\n",
    "train_data=Loader('./'+ DATA_FOLDER +'/','train')\n",
    "test_data=Loader('./'+ DATA_FOLDER +'/','test')\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\n",
    "print (train_loader)\n",
    "print (test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "    \n",
    "        #####---1\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0, bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.norm1 = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1.0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        #####\n",
    "        \n",
    "        #####---2\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=(2, 2) ,bias=True, groups=2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.norm2 = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1.0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        #####\n",
    "        \n",
    "        #####---3\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=(3, 3), padding=(1, 1), bias=True, groups=2)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=(1, 1), bias=True, groups=2)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3 , stride=2, padding=0)\n",
    "        #####\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_features=1536, out_features=4096, bias=True)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=256, bias=False)\n",
    "        self.fc9 = nn.Linear(in_features=256, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ###-----1------\n",
    "        out = self.conv1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool1(out)        \n",
    "        out = self.norm1(out)\n",
    "        #out = F.dropout(out, p=0.5)\n",
    "        \n",
    "        ###-----2------\n",
    "        out = self.conv2(out)\n",
    "        #out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.norm2(out)\n",
    "        #out = F.dropout(out, p=0.5)\n",
    "        \n",
    "        ###-----3------\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        ###-----4------\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool5(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) #flatten\n",
    "        out = self.fc6(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5)\n",
    "        \n",
    "        out = self.fc7(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5)\n",
    "      \n",
    "        out = self.fc8(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5)\n",
    "        \n",
    "        out = self.fc9(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DeepConv(epoch):\n",
    "    model_DeepConvNet.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.to(device=device, dtype=torch.float), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model_DeepConvNet(data)\n",
    "        target = target.float()\n",
    "        loss_d = Loss(output[0], target[0])\n",
    "        loss_phi = Loss(output[1], target[1])\n",
    "        loss = loss_d + loss_phi\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    TRAIN_LOSS.append(round((loss.data).cpu().numpy().tolist(),6))\n",
    "   \n",
    "    print('Train Epoch: {} \\t Loss_d: {} Loss_phi:{}'.format(\n",
    "            epoch, loss_d.data , loss_phi.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DeepConv(epoch):\n",
    "    model_DeepConvNet.eval()\n",
    "    test_loss_d = 0.0\n",
    "    test_loss_phi = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.to(device=device, dtype=torch.float), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model_DeepConvNet(data)\n",
    "        target = target.float()\n",
    "        test_loss_d = Loss(output[0], target[0])\n",
    "        test_loss_phi = Loss(output[1], target[1])\n",
    "        \n",
    "    print('---------------Test set: Average loss d:{}\\t loss phi:{}-------------------\\n'\n",
    "          .format(test_loss_d,test_loss_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepConvNet(\n",
      "  (conv1): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), bias=False)\n",
      "  (batchnorm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n",
      "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  (pool5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc6): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc8): Linear(in_features=4096, out_features=256, bias=False)\n",
      "  (fc9): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_DeepConvNet = DeepConvNet().cuda(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    model_DeepConvNet.to(device) \n",
    "#------------define optimizer/loss function\n",
    "Loss = nn.MSELoss(reduction='mean')    \n",
    "optimizer = torch.optim.SGD(model_DeepConvNet.parameters(), lr=LR, momentum=0.9, \n",
    "                            dampening=0, weight_decay=0.0005, nesterov=False)\n",
    "print(model_DeepConvNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t Loss_d: 0.17558278143405914 Loss_phi:0.09272637218236923\n",
      "---------------Test set: Average loss d:0.2634773254394531\t loss phi:0.22268258035182953-------------------\n",
      "\n",
      "Train Epoch: 2 \t Loss_d: 0.1837119311094284 Loss_phi:0.08929529041051865\n",
      "---------------Test set: Average loss d:0.14850912988185883\t loss phi:0.12020845711231232-------------------\n",
      "\n",
      "Train Epoch: 3 \t Loss_d: 0.21199269592761993 Loss_phi:0.10472551733255386\n",
      "---------------Test set: Average loss d:0.09001616388559341\t loss phi:0.07246122509241104-------------------\n",
      "\n",
      "Train Epoch: 4 \t Loss_d: 0.2673106789588928 Loss_phi:0.14970512688159943\n",
      "---------------Test set: Average loss d:0.0608973503112793\t loss phi:0.034182358533144-------------------\n",
      "\n",
      "Train Epoch: 5 \t Loss_d: 0.2757493555545807 Loss_phi:0.1578560769557953\n",
      "---------------Test set: Average loss d:0.03923690319061279\t loss phi:0.030616292729973793-------------------\n",
      "\n",
      "Train Epoch: 6 \t Loss_d: 0.2769702672958374 Loss_phi:0.16491112112998962\n",
      "---------------Test set: Average loss d:0.040383122861385345\t loss phi:0.03006112203001976-------------------\n",
      "\n",
      "Train Epoch: 7 \t Loss_d: 0.27392467856407166 Loss_phi:0.1650703102350235\n",
      "---------------Test set: Average loss d:0.03872989863157272\t loss phi:0.030106883496046066-------------------\n",
      "\n",
      "Train Epoch: 8 \t Loss_d: 0.27383163571357727 Loss_phi:0.16205087304115295\n",
      "---------------Test set: Average loss d:0.03872767090797424\t loss phi:0.030164441093802452-------------------\n",
      "\n",
      "Train Epoch: 9 \t Loss_d: 0.2787879407405853 Loss_phi:0.16447272896766663\n",
      "---------------Test set: Average loss d:0.03898495435714722\t loss phi:0.030000681057572365-------------------\n",
      "\n",
      "Train Epoch: 10 \t Loss_d: 0.2784549295902252 Loss_phi:0.16443943977355957\n",
      "---------------Test set: Average loss d:0.038774698972702026\t loss phi:0.02999262511730194-------------------\n",
      "\n",
      "Train Epoch: 11 \t Loss_d: 0.27794551849365234 Loss_phi:0.16437780857086182\n",
      "---------------Test set: Average loss d:0.03872464597225189\t loss phi:0.030013781040906906-------------------\n",
      "\n",
      "Train Epoch: 12 \t Loss_d: 0.2797345817089081 Loss_phi:0.1649344265460968\n",
      "---------------Test set: Average loss d:0.038742464035749435\t loss phi:0.030014069750905037-------------------\n",
      "\n",
      "Train Epoch: 13 \t Loss_d: 0.27871719002723694 Loss_phi:0.16507050395011902\n",
      "---------------Test set: Average loss d:0.03869885206222534\t loss phi:0.030052537098526955-------------------\n",
      "\n",
      "Train Epoch: 14 \t Loss_d: 0.2792622148990631 Loss_phi:0.1650909036397934\n",
      "---------------Test set: Average loss d:0.03874880075454712\t loss phi:0.029993068426847458-------------------\n",
      "\n",
      "Train Epoch: 15 \t Loss_d: 0.27896052598953247 Loss_phi:0.16510449349880219\n",
      "---------------Test set: Average loss d:0.038716260343790054\t loss phi:0.03001929074525833-------------------\n",
      "\n",
      "Train Epoch: 16 \t Loss_d: 0.27791160345077515 Loss_phi:0.16511666774749756\n",
      "---------------Test set: Average loss d:0.03868965804576874\t loss phi:0.02999240532517433-------------------\n",
      "\n",
      "Train Epoch: 17 \t Loss_d: 0.27927708625793457 Loss_phi:0.16490359604358673\n",
      "---------------Test set: Average loss d:0.03871752694249153\t loss phi:0.0301193967461586-------------------\n",
      "\n",
      "Train Epoch: 18 \t Loss_d: 0.2795335054397583 Loss_phi:0.16502508521080017\n",
      "---------------Test set: Average loss d:0.03868122771382332\t loss phi:0.030033160001039505-------------------\n",
      "\n",
      "Train Epoch: 19 \t Loss_d: 0.27972275018692017 Loss_phi:0.16511155664920807\n",
      "---------------Test set: Average loss d:0.038779884576797485\t loss phi:0.029993437230587006-------------------\n",
      "\n",
      "Train Epoch: 20 \t Loss_d: 0.27815544605255127 Loss_phi:0.1650690734386444\n",
      "---------------Test set: Average loss d:0.03873801603913307\t loss phi:0.02999473549425602-------------------\n",
      "\n",
      "Train Epoch: 21 \t Loss_d: 0.2793329954147339 Loss_phi:0.1651143878698349\n",
      "---------------Test set: Average loss d:0.03869088366627693\t loss phi:0.02999214641749859-------------------\n",
      "\n",
      "Train Epoch: 22 \t Loss_d: 0.2795156240463257 Loss_phi:0.1651163548231125\n",
      "---------------Test set: Average loss d:0.03876558691263199\t loss phi:0.029991542920470238-------------------\n",
      "\n",
      "Train Epoch: 23 \t Loss_d: 0.2796773910522461 Loss_phi:0.16511473059654236\n",
      "---------------Test set: Average loss d:0.03874154016375542\t loss phi:0.030040839686989784-------------------\n",
      "\n",
      "Train Epoch: 24 \t Loss_d: 0.27777040004730225 Loss_phi:0.16500268876552582\n",
      "---------------Test set: Average loss d:0.038864247500896454\t loss phi:0.030130552127957344-------------------\n",
      "\n",
      "Train Epoch: 25 \t Loss_d: 0.279436320066452 Loss_phi:0.16473248600959778\n",
      "---------------Test set: Average loss d:0.03878192603588104\t loss phi:0.029990801587700844-------------------\n",
      "\n",
      "Train Epoch: 26 \t Loss_d: 0.27855929732322693 Loss_phi:0.16481973230838776\n",
      "---------------Test set: Average loss d:0.03875206410884857\t loss phi:0.03001876175403595-------------------\n",
      "\n",
      "Train Epoch: 27 \t Loss_d: 0.27809369564056396 Loss_phi:0.1648070365190506\n",
      "---------------Test set: Average loss d:0.03889302909374237\t loss phi:0.03005935437977314-------------------\n",
      "\n",
      "Train Epoch: 28 \t Loss_d: 0.2773731052875519 Loss_phi:0.16187623143196106\n",
      "---------------Test set: Average loss d:0.03869400918483734\t loss phi:0.03000732511281967-------------------\n",
      "\n",
      "Train Epoch: 29 \t Loss_d: 0.2762313187122345 Loss_phi:0.1650763899087906\n",
      "---------------Test set: Average loss d:0.03874608874320984\t loss phi:0.030136054381728172-------------------\n",
      "\n",
      "Train Epoch: 30 \t Loss_d: 0.2772141695022583 Loss_phi:0.1645878702402115\n",
      "---------------Test set: Average loss d:0.03870469331741333\t loss phi:0.03008800372481346-------------------\n",
      "\n",
      "Train Epoch: 31 \t Loss_d: 0.27656611800193787 Loss_phi:0.16498686373233795\n",
      "---------------Test set: Average loss d:0.03871440887451172\t loss phi:0.030285555869340897-------------------\n",
      "\n",
      "Train Epoch: 32 \t Loss_d: 0.26874569058418274 Loss_phi:0.164473295211792\n",
      "---------------Test set: Average loss d:0.038722243160009384\t loss phi:0.03004862740635872-------------------\n",
      "\n",
      "Train Epoch: 33 \t Loss_d: 0.27914196252822876 Loss_phi:0.16407935321331024\n",
      "---------------Test set: Average loss d:0.03892027586698532\t loss phi:0.03003774769604206-------------------\n",
      "\n",
      "Train Epoch: 34 \t Loss_d: 0.2785514295101166 Loss_phi:0.16456428170204163\n",
      "---------------Test set: Average loss d:0.038900528103113174\t loss phi:0.030364179983735085-------------------\n",
      "\n",
      "Train Epoch: 35 \t Loss_d: 0.2689593732357025 Loss_phi:0.16472044587135315\n",
      "---------------Test set: Average loss d:0.03894978016614914\t loss phi:0.030128680169582367-------------------\n",
      "\n",
      "Train Epoch: 36 \t Loss_d: 0.27891215682029724 Loss_phi:0.16511306166648865\n",
      "---------------Test set: Average loss d:0.038788117468357086\t loss phi:0.02999045141041279-------------------\n",
      "\n",
      "Train Epoch: 37 \t Loss_d: 0.27971163392066956 Loss_phi:0.16502989828586578\n",
      "---------------Test set: Average loss d:0.03868529945611954\t loss phi:0.0299928430467844-------------------\n",
      "\n",
      "Train Epoch: 38 \t Loss_d: 0.27960386872291565 Loss_phi:0.16495585441589355\n",
      "---------------Test set: Average loss d:0.03868213668465614\t loss phi:0.029997093603014946-------------------\n",
      "\n",
      "Train Epoch: 39 \t Loss_d: 0.27945101261138916 Loss_phi:0.16511349380016327\n",
      "---------------Test set: Average loss d:0.03868129104375839\t loss phi:0.029990339651703835-------------------\n",
      "\n",
      "Train Epoch: 40 \t Loss_d: 0.2793431580066681 Loss_phi:0.16503700613975525\n",
      "---------------Test set: Average loss d:0.03868315741419792\t loss phi:0.03007218800485134-------------------\n",
      "\n",
      "Train Epoch: 41 \t Loss_d: 0.2794489562511444 Loss_phi:0.16503801941871643\n",
      "---------------Test set: Average loss d:0.03872358053922653\t loss phi:0.03001384064555168-------------------\n",
      "\n",
      "Train Epoch: 42 \t Loss_d: 0.2794344425201416 Loss_phi:0.16499483585357666\n",
      "---------------Test set: Average loss d:0.038681063801050186\t loss phi:0.029991360381245613-------------------\n",
      "\n",
      "Train Epoch: 43 \t Loss_d: 0.2797730267047882 Loss_phi:0.16511380672454834\n",
      "---------------Test set: Average loss d:0.038680657744407654\t loss phi:0.029990900307893753-------------------\n",
      "\n",
      "Train Epoch: 44 \t Loss_d: 0.27971765398979187 Loss_phi:0.16511359810829163\n",
      "---------------Test set: Average loss d:0.038680873811244965\t loss phi:0.029990527778863907-------------------\n",
      "\n",
      "Train Epoch: 45 \t Loss_d: 0.2793394923210144 Loss_phi:0.16512008011341095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Test set: Average loss d:0.03868221491575241\t loss phi:0.03000873327255249-------------------\n",
      "\n",
      "Train Epoch: 46 \t Loss_d: 0.2796795964241028 Loss_phi:0.1651197373867035\n",
      "---------------Test set: Average loss d:0.0386832021176815\t loss phi:0.030003421008586884-------------------\n",
      "\n",
      "Train Epoch: 47 \t Loss_d: 0.27955788373947144 Loss_phi:0.1651199460029602\n",
      "---------------Test set: Average loss d:0.0386803075671196\t loss phi:0.02999044954776764-------------------\n",
      "\n",
      "Train Epoch: 48 \t Loss_d: 0.27882111072540283 Loss_phi:0.16511417925357819\n",
      "---------------Test set: Average loss d:0.03869400545954704\t loss phi:0.029990561306476593-------------------\n",
      "\n",
      "Train Epoch: 49 \t Loss_d: 0.2796659469604492 Loss_phi:0.16508431732654572\n",
      "---------------Test set: Average loss d:0.03868115693330765\t loss phi:0.029999498277902603-------------------\n",
      "\n",
      "Train Epoch: 50 \t Loss_d: 0.279621958732605 Loss_phi:0.16512008011341095\n",
      "---------------Test set: Average loss d:0.038681380450725555\t loss phi:0.02999076247215271-------------------\n",
      "\n",
      "Train Epoch: 51 \t Loss_d: 0.27973857522010803 Loss_phi:0.16512003540992737\n",
      "---------------Test set: Average loss d:0.038682468235492706\t loss phi:0.02999671921133995-------------------\n",
      "\n",
      "Train Epoch: 52 \t Loss_d: 0.2794741690158844 Loss_phi:0.1651199758052826\n",
      "---------------Test set: Average loss d:0.03870490938425064\t loss phi:0.029990244656801224-------------------\n",
      "\n",
      "Train Epoch: 53 \t Loss_d: 0.27975815534591675 Loss_phi:0.1651163101196289\n",
      "---------------Test set: Average loss d:0.03870731219649315\t loss phi:0.029991663992404938-------------------\n",
      "\n",
      "Train Epoch: 54 \t Loss_d: 0.2797873318195343 Loss_phi:0.16511914134025574\n",
      "---------------Test set: Average loss d:0.03868674486875534\t loss phi:0.029991760849952698-------------------\n",
      "\n",
      "Train Epoch: 55 \t Loss_d: 0.27978307008743286 Loss_phi:0.16512008011341095\n",
      "---------------Test set: Average loss d:0.038680605590343475\t loss phi:0.02999279275536537-------------------\n",
      "\n",
      "Train Epoch: 56 \t Loss_d: 0.27972403168678284 Loss_phi:0.16512008011341095\n",
      "---------------Test set: Average loss d:0.03868551552295685\t loss phi:0.029990270733833313-------------------\n",
      "\n",
      "Train Epoch: 57 \t Loss_d: 0.27926138043403625 Loss_phi:0.1651201844215393\n",
      "---------------Test set: Average loss d:0.0386827178299427\t loss phi:0.03000137209892273-------------------\n",
      "\n",
      "Train Epoch: 58 \t Loss_d: 0.2797339856624603 Loss_phi:0.16512010991573334\n",
      "---------------Test set: Average loss d:0.03868071734905243\t loss phi:0.029990607872605324-------------------\n",
      "\n",
      "Train Epoch: 59 \t Loss_d: 0.2797147333621979 Loss_phi:0.16508497297763824\n",
      "---------------Test set: Average loss d:0.03872689977288246\t loss phi:0.02999076619744301-------------------\n",
      "\n",
      "Train Epoch: 60 \t Loss_d: 0.27970245480537415 Loss_phi:0.1651201844215393\n",
      "---------------Test set: Average loss d:0.03868190571665764\t loss phi:0.030013814568519592-------------------\n",
      "\n",
      "Train Epoch: 61 \t Loss_d: 0.27839985489845276 Loss_phi:0.1651202142238617\n",
      "---------------Test set: Average loss d:0.03868187591433525\t loss phi:0.029991021379828453-------------------\n",
      "\n",
      "Train Epoch: 62 \t Loss_d: 0.27952346205711365 Loss_phi:0.16511894762516022\n",
      "---------------Test set: Average loss d:0.03868282586336136\t loss phi:0.02999204210937023-------------------\n",
      "\n",
      "Train Epoch: 63 \t Loss_d: 0.2794712483882904 Loss_phi:0.16510260105133057\n",
      "---------------Test set: Average loss d:0.03868170082569122\t loss phi:0.0299912728369236-------------------\n",
      "\n",
      "Train Epoch: 64 \t Loss_d: 0.2796916663646698 Loss_phi:0.16512000560760498\n",
      "---------------Test set: Average loss d:0.038681186735630035\t loss phi:0.029991090297698975-------------------\n",
      "\n",
      "Train Epoch: 65 \t Loss_d: 0.2795919179916382 Loss_phi:0.16503313183784485\n",
      "---------------Test set: Average loss d:0.038680560886859894\t loss phi:0.029990455135703087-------------------\n",
      "\n",
      "Train Epoch: 66 \t Loss_d: 0.27888405323028564 Loss_phi:0.16511912643909454\n",
      "---------------Test set: Average loss d:0.03868011012673378\t loss phi:0.02999042719602585-------------------\n",
      "\n",
      "Train Epoch: 67 \t Loss_d: 0.27194592356681824 Loss_phi:0.1651194989681244\n",
      "---------------Test set: Average loss d:0.03868076577782631\t loss phi:0.030003370717167854-------------------\n",
      "\n",
      "Train Epoch: 68 \t Loss_d: 0.2780972123146057 Loss_phi:0.16511745750904083\n",
      "---------------Test set: Average loss d:0.038681402802467346\t loss phi:0.029991447925567627-------------------\n",
      "\n",
      "Train Epoch: 69 \t Loss_d: 0.27967843413352966 Loss_phi:0.16511577367782593\n",
      "---------------Test set: Average loss d:0.03870994970202446\t loss phi:0.029997678473591805-------------------\n",
      "\n",
      "Train Epoch: 70 \t Loss_d: 0.2797584533691406 Loss_phi:0.16512003540992737\n",
      "---------------Test set: Average loss d:0.03868187963962555\t loss phi:0.02999032475054264-------------------\n",
      "\n",
      "Train Epoch: 71 \t Loss_d: 0.27974697947502136 Loss_phi:0.1651202142238617\n",
      "---------------Test set: Average loss d:0.038680270314216614\t loss phi:0.029991278424859047-------------------\n",
      "\n",
      "Train Epoch: 72 \t Loss_d: 0.2794151306152344 Loss_phi:0.16511507332324982\n",
      "---------------Test set: Average loss d:0.03869037702679634\t loss phi:0.02999168448150158-------------------\n",
      "\n",
      "Train Epoch: 73 \t Loss_d: 0.27974289655685425 Loss_phi:0.16489781439304352\n",
      "---------------Test set: Average loss d:0.03868040814995766\t loss phi:0.029990823939442635-------------------\n",
      "\n",
      "Train Epoch: 74 \t Loss_d: 0.2794016897678375 Loss_phi:0.1650783121585846\n",
      "---------------Test set: Average loss d:0.03868594765663147\t loss phi:0.02999088540673256-------------------\n",
      "\n",
      "Train Epoch: 75 \t Loss_d: 0.2795049548149109 Loss_phi:0.16508913040161133\n",
      "---------------Test set: Average loss d:0.038680512458086014\t loss phi:0.029993966221809387-------------------\n",
      "\n",
      "Train Epoch: 76 \t Loss_d: 0.27803462743759155 Loss_phi:0.16511324048042297\n",
      "---------------Test set: Average loss d:0.03868059441447258\t loss phi:0.0299903005361557-------------------\n",
      "\n",
      "Train Epoch: 77 \t Loss_d: 0.2791968882083893 Loss_phi:0.16506659984588623\n",
      "---------------Test set: Average loss d:0.03868408501148224\t loss phi:0.029990237206220627-------------------\n",
      "\n",
      "Train Epoch: 78 \t Loss_d: 0.2797459065914154 Loss_phi:0.16511979699134827\n",
      "---------------Test set: Average loss d:0.038691774010658264\t loss phi:0.03000711463391781-------------------\n",
      "\n",
      "Train Epoch: 79 \t Loss_d: 0.2797299325466156 Loss_phi:0.16500850021839142\n",
      "---------------Test set: Average loss d:0.038683220744132996\t loss phi:0.03000902570784092-------------------\n",
      "\n",
      "Train Epoch: 80 \t Loss_d: 0.27976274490356445 Loss_phi:0.16511444747447968\n",
      "---------------Test set: Average loss d:0.038680680096149445\t loss phi:0.030005061998963356-------------------\n",
      "\n",
      "Train Epoch: 81 \t Loss_d: 0.2792656123638153 Loss_phi:0.1651201844215393\n",
      "---------------Test set: Average loss d:0.03868107870221138\t loss phi:0.029995666816830635-------------------\n",
      "\n",
      "Train Epoch: 82 \t Loss_d: 0.27973827719688416 Loss_phi:0.16509152948856354\n",
      "---------------Test set: Average loss d:0.038682401180267334\t loss phi:0.029990412294864655-------------------\n",
      "\n",
      "Train Epoch: 83 \t Loss_d: 0.27938902378082275 Loss_phi:0.16512013971805573\n",
      "---------------Test set: Average loss d:0.03869011998176575\t loss phi:0.029991446062922478-------------------\n",
      "\n",
      "Train Epoch: 84 \t Loss_d: 0.27937573194503784 Loss_phi:0.16511116921901703\n",
      "---------------Test set: Average loss d:0.03881409764289856\t loss phi:0.02999032847583294-------------------\n",
      "\n",
      "Train Epoch: 85 \t Loss_d: 0.2794628441333771 Loss_phi:0.16511662304401398\n",
      "---------------Test set: Average loss d:0.038680803030729294\t loss phi:0.02999543398618698-------------------\n",
      "\n",
      "Train Epoch: 86 \t Loss_d: 0.2778817117214203 Loss_phi:0.16407917439937592\n",
      "---------------Test set: Average loss d:0.03868890553712845\t loss phi:0.029999082908034325-------------------\n",
      "\n",
      "Train Epoch: 87 \t Loss_d: 0.27854248881340027 Loss_phi:0.16508617997169495\n",
      "---------------Test set: Average loss d:0.03887685388326645\t loss phi:0.029998790472745895-------------------\n",
      "\n",
      "Train Epoch: 88 \t Loss_d: 0.27959686517715454 Loss_phi:0.16509900987148285\n",
      "---------------Test set: Average loss d:0.03869267925620079\t loss phi:0.029997585341334343-------------------\n",
      "\n",
      "Train Epoch: 89 \t Loss_d: 0.27924349904060364 Loss_phi:0.16511137783527374\n",
      "---------------Test set: Average loss d:0.038708772510290146\t loss phi:0.02999645471572876-------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 \t Loss_d: 0.2783358097076416 Loss_phi:0.16477864980697632\n",
      "---------------Test set: Average loss d:0.038681771606206894\t loss phi:0.03000093810260296-------------------\n",
      "\n",
      "Train Epoch: 91 \t Loss_d: 0.279507040977478 Loss_phi:0.1651025414466858\n",
      "---------------Test set: Average loss d:0.0386890210211277\t loss phi:0.029990551993250847-------------------\n",
      "\n",
      "Train Epoch: 92 \t Loss_d: 0.27962401509284973 Loss_phi:0.16509725153446198\n",
      "---------------Test set: Average loss d:0.03869175165891647\t loss phi:0.02999105304479599-------------------\n",
      "\n",
      "Train Epoch: 93 \t Loss_d: 0.27973949909210205 Loss_phi:0.1649307906627655\n",
      "---------------Test set: Average loss d:0.03868034854531288\t loss phi:0.029990535229444504-------------------\n",
      "\n",
      "Train Epoch: 94 \t Loss_d: 0.27938494086265564 Loss_phi:0.16506506502628326\n",
      "---------------Test set: Average loss d:0.03869378939270973\t loss phi:0.02999028004705906-------------------\n",
      "\n",
      "Train Epoch: 95 \t Loss_d: 0.2758478820323944 Loss_phi:0.1649831086397171\n",
      "---------------Test set: Average loss d:0.038708657026290894\t loss phi:0.029993241652846336-------------------\n",
      "\n",
      "Train Epoch: 96 \t Loss_d: 0.2796382009983063 Loss_phi:0.1650574803352356\n",
      "---------------Test set: Average loss d:0.03868502378463745\t loss phi:0.029990488663315773-------------------\n",
      "\n",
      "Train Epoch: 97 \t Loss_d: 0.27733343839645386 Loss_phi:0.16511373221874237\n",
      "---------------Test set: Average loss d:0.03868013992905617\t loss phi:0.030003758147358894-------------------\n",
      "\n",
      "Train Epoch: 98 \t Loss_d: 0.2748503088951111 Loss_phi:0.16510222852230072\n",
      "---------------Test set: Average loss d:0.038686905056238174\t loss phi:0.029991477727890015-------------------\n",
      "\n",
      "Train Epoch: 99 \t Loss_d: 0.2794671654701233 Loss_phi:0.1651197373867035\n",
      "---------------Test set: Average loss d:0.03868427127599716\t loss phi:0.029996035620570183-------------------\n",
      "\n",
      "Train Epoch: 100 \t Loss_d: 0.2736784815788269 Loss_phi:0.1651090681552887\n",
      "---------------Test set: Average loss d:0.038927529007196426\t loss phi:0.03002568706870079-------------------\n",
      "\n",
      "Train Epoch: 101 \t Loss_d: 0.2789615988731384 Loss_phi:0.16509126126766205\n",
      "---------------Test set: Average loss d:0.03868473321199417\t loss phi:0.030007701367139816-------------------\n",
      "\n",
      "Train Epoch: 102 \t Loss_d: 0.2792230248451233 Loss_phi:0.16509601473808289\n",
      "---------------Test set: Average loss d:0.03871035575866699\t loss phi:0.030154570937156677-------------------\n",
      "\n",
      "Train Epoch: 103 \t Loss_d: 0.2795521318912506 Loss_phi:0.1651201844215393\n",
      "---------------Test set: Average loss d:0.038697145879268646\t loss phi:0.029994940385222435-------------------\n",
      "\n",
      "Train Epoch: 104 \t Loss_d: 0.277889221906662 Loss_phi:0.16511918604373932\n",
      "---------------Test set: Average loss d:0.0386892706155777\t loss phi:0.02999209426343441-------------------\n",
      "\n",
      "Train Epoch: 105 \t Loss_d: 0.27960488200187683 Loss_phi:0.1651143878698349\n",
      "---------------Test set: Average loss d:0.038680072873830795\t loss phi:0.02999192476272583-------------------\n",
      "\n",
      "Train Epoch: 106 \t Loss_d: 0.27972400188446045 Loss_phi:0.16511990129947662\n",
      "---------------Test set: Average loss d:0.038689661771059036\t loss phi:0.03000420331954956-------------------\n",
      "\n",
      "Train Epoch: 107 \t Loss_d: 0.2796500325202942 Loss_phi:0.16511303186416626\n",
      "---------------Test set: Average loss d:0.03869134187698364\t loss phi:0.030003074556589127-------------------\n",
      "\n",
      "Train Epoch: 108 \t Loss_d: 0.27971285581588745 Loss_phi:0.1651199460029602\n",
      "---------------Test set: Average loss d:0.03868011385202408\t loss phi:0.029995298013091087-------------------\n",
      "\n",
      "Train Epoch: 109 \t Loss_d: 0.27893492579460144 Loss_phi:0.16511772572994232\n",
      "---------------Test set: Average loss d:0.038686446845531464\t loss phi:0.029990263283252716-------------------\n",
      "\n",
      "Train Epoch: 110 \t Loss_d: 0.27925071120262146 Loss_phi:0.16512013971805573\n",
      "---------------Test set: Average loss d:0.038691312074661255\t loss phi:0.029992563650012016-------------------\n",
      "\n",
      "Train Epoch: 111 \t Loss_d: 0.2795294523239136 Loss_phi:0.16511164605617523\n",
      "---------------Test set: Average loss d:0.03868066519498825\t loss phi:0.02999025769531727-------------------\n",
      "\n",
      "Train Epoch: 112 \t Loss_d: 0.27967384457588196 Loss_phi:0.16506202518939972\n",
      "---------------Test set: Average loss d:0.03869292885065079\t loss phi:0.02999180741608143-------------------\n",
      "\n",
      "Train Epoch: 113 \t Loss_d: 0.27956312894821167 Loss_phi:0.16512003540992737\n",
      "---------------Test set: Average loss d:0.03868211433291435\t loss phi:0.029990311712026596-------------------\n",
      "\n",
      "Train Epoch: 114 \t Loss_d: 0.2792273163795471 Loss_phi:0.16511349380016327\n",
      "---------------Test set: Average loss d:0.03868057578802109\t loss phi:0.02999032475054264-------------------\n",
      "\n",
      "Train Epoch: 115 \t Loss_d: 0.2796971797943115 Loss_phi:0.16509824991226196\n",
      "---------------Test set: Average loss d:0.038682300597429276\t loss phi:0.029991108924150467-------------------\n",
      "\n",
      "Train Epoch: 116 \t Loss_d: 0.2796153426170349 Loss_phi:0.16511741280555725\n",
      "---------------Test set: Average loss d:0.03868022933602333\t loss phi:0.02999098040163517-------------------\n",
      "\n",
      "Train Epoch: 117 \t Loss_d: 0.2796972393989563 Loss_phi:0.16511906683444977\n",
      "---------------Test set: Average loss d:0.038682833313941956\t loss phi:0.029992355033755302-------------------\n",
      "\n",
      "Train Epoch: 118 \t Loss_d: 0.279634028673172 Loss_phi:0.1651202142238617\n",
      "---------------Test set: Average loss d:0.038691446185112\t loss phi:0.02999022975564003-------------------\n",
      "\n",
      "Train Epoch: 119 \t Loss_d: 0.27978020906448364 Loss_phi:0.16512013971805573\n",
      "---------------Test set: Average loss d:0.03868008404970169\t loss phi:0.02999022603034973-------------------\n",
      "\n",
      "Train Epoch: 120 \t Loss_d: 0.2784596085548401 Loss_phi:0.1651202142238617\n",
      "---------------Test set: Average loss d:0.03868011012673378\t loss phi:0.029990481212735176-------------------\n",
      "\n",
      "Train Epoch: 121 \t Loss_d: 0.2797640860080719 Loss_phi:0.1650516390800476\n",
      "---------------Test set: Average loss d:0.038680851459503174\t loss phi:0.02999104931950569-------------------\n",
      "\n",
      "Train Epoch: 122 \t Loss_d: 0.27975285053253174 Loss_phi:0.16494865715503693\n",
      "---------------Test set: Average loss d:0.0386800691485405\t loss phi:0.029990827664732933-------------------\n",
      "\n",
      "Train Epoch: 123 \t Loss_d: 0.2797866463661194 Loss_phi:0.1651202142238617\n",
      "---------------Test set: Average loss d:0.03868410736322403\t loss phi:0.029990343376994133-------------------\n",
      "\n",
      "Train Epoch: 124 \t Loss_d: 0.2797292172908783 Loss_phi:0.16511987149715424\n",
      "---------------Test set: Average loss d:0.038680095225572586\t loss phi:0.02999097853899002-------------------\n",
      "\n",
      "Train Epoch: 125 \t Loss_d: 0.279568076133728 Loss_phi:0.1651201844215393\n",
      "---------------Test set: Average loss d:0.03868250548839569\t loss phi:0.0299905426800251-------------------\n",
      "\n",
      "Train Epoch: 126 \t Loss_d: 0.2793014347553253 Loss_phi:0.16511565446853638\n",
      "---------------Test set: Average loss d:0.03868013620376587\t loss phi:0.029991284012794495-------------------\n",
      "\n",
      "Train Epoch: 127 \t Loss_d: 0.27938541769981384 Loss_phi:0.16452236473560333\n",
      "---------------Test set: Average loss d:0.038680776953697205\t loss phi:0.02999027818441391-------------------\n",
      "\n",
      "Train Epoch: 128 \t Loss_d: 0.27966222167015076 Loss_phi:0.1651199758052826\n",
      "---------------Test set: Average loss d:0.03868257626891136\t loss phi:0.029992030933499336-------------------\n",
      "\n",
      "Train Epoch: 129 \t Loss_d: 0.2775641679763794 Loss_phi:0.16510970890522003\n",
      "---------------Test set: Average loss d:0.03872067108750343\t loss phi:0.029991239309310913-------------------\n",
      "\n",
      "Train Epoch: 130 \t Loss_d: 0.2793412208557129 Loss_phi:0.16512013971805573\n",
      "---------------Test set: Average loss d:0.038681644946336746\t loss phi:0.029991140589118004-------------------\n",
      "\n",
      "Train Epoch: 131 \t Loss_d: 0.2797829806804657 Loss_phi:0.16512000560760498\n",
      "---------------Test set: Average loss d:0.038683753460645676\t loss phi:0.02999025769531727-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#---------------Train-----------------\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    INDEX.append(epoch)\n",
    "    train_DeepConv(epoch)\n",
    "    test_DeepConv(epoch)\n",
    "    if (epoch%100==0):\n",
    "        PATH =str(epoch)+'model.pkl'\n",
    "        torch.save(model_DeepConvNet.state_dict(), PATH)\n",
    "    '''\n",
    "    savefilename = str(epoch)+'.tar'\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model_EEGNet.state_dict(),\n",
    "        }, savefilename)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
